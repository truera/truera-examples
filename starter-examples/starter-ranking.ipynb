{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Exercise: Ranking\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/truera-examples/blob/release/prod/starter-examples/starter-ranking.ipynb)\n",
    "\n",
    "### Setup\n",
    "\n",
    "You are a data scientist working for a newly opened winery. The winery is preparing for a big tasting event that will be attended by several eminent wine critics. To help prepare for the event, you are tasked with training a model that can rank wines for a given wine critic.  \n",
    "\n",
    "Fortunately, you have historical data for these authors' wine reviews. For each review written, the critic assigns the wine in question a numerical score (from 0 to 100). The data includes features relevant to the wine itself (e.g., the country of origin, the bottling year, the price of the wine) and to the critic (e.g., the critic's average review length, whether or not they use Twitter).\n",
    "\n",
    "Your model will use this information to rank a selection of your winery's wines. The intent of this ranking is to craft an individualized tasting menu for each critic attending your winery's event.\n",
    "\n",
    "#### Goals ðŸŽ¯\n",
    "In this tutorial you will learn how to:\n",
    "1. Set up and ingest a ranking project into TruEra Diagnostics. \n",
    "2. Change the ranking project setting for `K` (i.e., the number of elements per group ID).\n",
    "\n",
    "### First, set the credentials for your TruEra deployment.\n",
    "If you don't have credentials yet, get them instantly by signing up for free at: https://www.truera.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection details\n",
    "CONNECTION_STRING = \"https://app.truera.net\"\n",
    "AUTH_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install truera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TruEra workspace.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRanker\n",
    "from xgboost import XGBRanker\n",
    "\n",
    "from truera.client.truera_workspace import TrueraWorkspace\n",
    "from truera.client.truera_authentication import TokenAuthentication\n",
    "from truera.client.ingestion.util import ColumnSpec\n",
    "\n",
    "auth = TokenAuthentication(AUTH_TOKEN)\n",
    "tru = TrueraWorkspace(CONNECTION_STRING, auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, run the rest of the notebook and follow the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, load the world-wines-ranking data\n",
    "\n",
    "We will use a subset of the [world-wines-ranking](https://www.kaggle.com/datasets/diegoperezsalas/worldwinesranking/) dataset. This data includes about 130k wine ratings from industry experts. We have cleaned, preprocessed, and downsampled this data to 6k records (1.5k train, 4.5k split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"https://truera-examples.s3.us-west-2.amazonaws.com/data/starter-ranking/\"\n",
    "split_names = [\"all\", \"train\", \"test\"]\n",
    "splits = {}\n",
    "for split_name in split_names:\n",
    "    split_path = dir_name + split_name + \".csv\"\n",
    "    print(f\"-> Loading {split_name} from {split_path}...\")\n",
    "    splits[split_name] = pd.read_csv(split_path)\n",
    "    splits[split_name]['taster_has_twitter'] = splits[split_name]['taster_has_twitter'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits['train'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the column names for the pre data and feature influences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names(df):\n",
    "    not_pre_cols = ['points', 'title', 'winery', 'taster_id', 'id']\n",
    "    pre_cols = []\n",
    "    for col in df.columns:\n",
    "        if col not in not_pre_cols:\n",
    "            pre_cols.append(col)\n",
    "    return pre_cols\n",
    "\n",
    "pre_cols = get_col_names(splits['all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking IDs: groups and items\n",
    "\n",
    "Ranking projects in TruEra require you to specify two additional columns:\n",
    "\n",
    "- `ranking_group_id_column`: indicates the group to which the record belongs (in this demo, the critic rating the wine). \n",
    "- `ranking_item_id_column`: indicates the item which the record is associate with (in this demo, the name of the wine).\n",
    "\n",
    "Both of these columns must be specified when defining your `ColumnSpec`. The group ID is relevant when `fit`ting your ranking model, as will become evident in the subsequent cells.\n",
    "\n",
    "For more details on these ranking columns, check out the [TruEra documentation](https://docs.truera.com/1.40/public/project-overview/) (see the \"Ranking\" modal under \"Understanding Output Types)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the `xgbranker` model\n",
    "\n",
    "Train a ranking model using the `xgboost` package. Note that this `fit` method is similar to an analogous classification/regression model's `fit` with the addition of a `qid` parameter. This paramater indicates the \"group ID\" of each record being passed in with the `X` and `Y` dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the data for use with the `XGBRanker` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_xgboost_data(df, pre_cols):\n",
    "    # sort by the group id; required for xgbranker model\n",
    "    df = df.sort_values(by=['taster_id'])\n",
    "    # use only pre data columns as features\n",
    "    X = df[pre_cols]\n",
    "    # get the relevance scores\n",
    "    Y = df[['points']]\n",
    "    # get the user (group) IDs as integer vals from the 'userId' col\n",
    "    qid = df['taster_id'].apply(lambda x: int(x[-2:])).astype(int)\n",
    "    return X, Y, qid\n",
    "\n",
    "X, Y, qid = df_to_xgboost_data(splits['train'], pre_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the XGBRanker model\n",
    "xgb_ranker = XGBRanker(tree_method=\"hist\",\n",
    "                   lambdarank_num_pair_per_sample=10,\n",
    "                   objective=\"rank:ndcg\",\n",
    "                   lambdarank_pair_method=\"topk\",\n",
    "                   random_state=1)\n",
    "xgb_ranker.fit(X, Y, qid=qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = 'pred_score_xgb'\n",
    "for split_name, split in splits.items():\n",
    "    Y_pred = xgb_ranker.predict(split[pre_cols])\n",
    "    splits[split_name][pred_xgb] = Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the `LGBMRanker` Model\n",
    "\n",
    "Train another ranking modeing the `lightgbm` package.\n",
    "\n",
    "There are some minor differences in using the `LGBMRanker`, namely:\n",
    "- The `group` parameter is the number of contiguous records belonging to the group. For the training data, this is always 100 records.\n",
    "- Relevance values (labels) must start from 0, so we subtract by the `min` value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert qid into lgbm-compatible groups\n",
    "group = [100 for id in np.unique(qid)]\n",
    "Y_lgbm = Y - Y.min()[0] # \n",
    "\n",
    "lgbm_ranker = LGBMRanker(random_state=0)\n",
    "lgbm_ranker.fit(\n",
    "    X,\n",
    "    Y_lgbm,\n",
    "    group=group,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgbm = 'pred_score_lgbm'\n",
    "for split_name, split in splits.items():\n",
    "    Y_pred = lgbm_ranker.predict(split[pre_cols])\n",
    "    splits[split_name][pred_lgbm] = Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting the data/models into TruEra\n",
    "\n",
    "Now we ingest the `world-wines-ranking` data and the trained models into TruEra. First we set up the names for the project's artifacts and the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names for project setup\n",
    "project_name = \"Starter - Ranking\"\n",
    "model_name_xgb = \"wine_xgbranker\"\n",
    "model_name_lgbm = \"wine_lgbmranker\"\n",
    "score_type = \"ranking_score\" # either \"ranking_score\" (raw model scores) or \"rank\" (rank-ordering of model scores); \n",
    "\n",
    "# add project + data collection\n",
    "tru.add_project(project_name, score_type)\n",
    "tru.add_data_collection(\"wine_rating_6k_ratings\")\n",
    "\n",
    "# reduce settings for speed\n",
    "tru.set_num_internal_qii_samples(100)\n",
    "tru.set_num_default_influences(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the column names for `ColumnSpec` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col_name = \"id\" \n",
    "ranking_item_id_column = \"title\" \n",
    "ranking_group_id_column = \"taster_id\" \n",
    "label_col_name = \"points\"\n",
    "extra_data_col_names = ['winery']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pre data/labels/extra data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_names = ['train', 'test', 'all'] # reorder these so we can add_python_model with train_split_name \n",
    "\n",
    "column_spec_no_preds = ColumnSpec(\n",
    "            id_col_name=id_col_name,\n",
    "            ranking_item_id_column_name=ranking_item_id_column,\n",
    "            ranking_group_id_column_name=ranking_group_id_column,\n",
    "            pre_data_col_names=pre_cols,\n",
    "            label_col_names=label_col_name,\n",
    "            extra_data_col_names=extra_data_col_names\n",
    "        )\n",
    "\n",
    "for data_split_name in split_names:\n",
    "    tru.add_data(\n",
    "        data=splits[data_split_name],\n",
    "        column_spec=column_spec_no_preds, \n",
    "        data_split_name=data_split_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add predictions (note: we manually add these since computation of ranking predictions in TruEra is currently not supported).\n",
    "\n",
    "We add the `train` split first since it is the training split for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_spec_for_preds(pred_name):\n",
    "    return ColumnSpec(\n",
    "            id_col_name=id_col_name,\n",
    "            ranking_item_id_column_name=ranking_item_id_column,\n",
    "            ranking_group_id_column_name=ranking_group_id_column,\n",
    "            prediction_col_names=[pred_name],\n",
    "        )\n",
    "\n",
    "for data_split_name in split_names:\n",
    "    for model_name, model, pred_col_name in zip([model_name_xgb, model_name_lgbm], [xgb_ranker, lgbm_ranker], [pred_xgb, pred_lgbm]):\n",
    "        if data_split_name == \"train\":\n",
    "            tru.add_python_model(model_name, model, train_split_name=\"train\")\n",
    "        tru.set_model(model_name)\n",
    "        tru.add_data(\n",
    "            data=splits[data_split_name][[id_col_name, pred_col_name, ranking_group_id_column, ranking_item_id_column]],\n",
    "            column_spec=column_spec_for_preds(pred_col_name), \n",
    "            data_split_name=data_split_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute influences for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_split_name in split_names:\n",
    "    for model_name in [model_name_xgb, model_name_lgbm]:\n",
    "        tru.set_model(model_name)\n",
    "        tru.set_data_split(data_split_name)\n",
    "        tru.compute_feature_influences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see the `wine_ratings` project in your list of TruEra projects now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing/Changing `K` for NDCG\n",
    "\n",
    "`K` is a project-level setting that is unique to ranking projects. Mainly, `K` dictates the number of records per group to consider when calculating the Normalized Discounted Cumulative Gain (NDCG) of a model on a split/segment. Read more about NDCG in the [TruEra documentation](https://docs.truera.com/1.40/public/supported-metrics/accuracy-metrics/#ranking-models).\n",
    "\n",
    "The following cells show you how to interact with this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default ranking K value\n",
    "tru.get_ranking_k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an explainer to compute NDCG\n",
    "explainer = tru.get_explainer()\n",
    "explainer.compute_performance(\"NDCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change this project setting as follows\n",
    "tru.set_ranking_k(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the NDCG should change for a different value of K\n",
    "explainer.compute_performance(\"NDCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the new value of K\n",
    "tru.get_ranking_k()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
