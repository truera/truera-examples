{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88137095",
   "metadata": {
    "id": "88137095"
   },
   "source": [
    "# Guided Exercise: Fairness\n",
    "\n",
    "### Goals ðŸŽ¯\n",
    "In this tutorial, you will view the fairness test results from part 1, identify the root cause and mitigate the fairness issues in the model.\n",
    "\n",
    "The data used is ACS Employment data made available through the [*folktables* repository](https://github.com/zykls/folktables)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/truera-examples/blob/release/prod/starter-examples/starter-fairness-part-2.ipynb)\n",
    "\n",
    "### First, set the credentials for your TruEra deployment.\n",
    "If you don't have credentials yet, get them instantly by signing up for free at: https://www.truera.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299bbe9",
   "metadata": {
    "id": "4299bbe9"
   },
   "outputs": [],
   "source": [
    "# connection details\n",
    "TRUERA_URL = \"https://app.truera.net/\"\n",
    "AUTH_TOKEN = \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce855c64",
   "metadata": {
    "id": "ce855c64"
   },
   "source": [
    "### Install required packages for running in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cc7f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b19cc7f6",
    "outputId": "114a8281-0938-44d3-f7d5-1f2b99829f28"
   },
   "outputs": [],
   "source": [
    "! pip install truera==12.1.0 pandas==1.5.2 sklearn==1.3.0 xgboost==1.6.2 smart-open==6.3.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f5e86b1",
   "metadata": {
    "id": "3f5e86b1"
   },
   "source": [
    "### From here, you can run the rest of the notebook to follow the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0de28",
   "metadata": {
    "id": "52a0de28"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample\n",
    "from truera.client.ingestion import ColumnSpec, ModelOutputContext\n",
    "from truera.client.truera_authentication import TokenAuthentication\n",
    "from truera.client.truera_workspace import TrueraWorkspace\n",
    "\n",
    "auth = TokenAuthentication(AUTH_TOKEN)\n",
    "tru = TrueraWorkspace(TRUERA_URL, auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5gnWRtDQNAOa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5gnWRtDQNAOa",
    "outputId": "c8b0af05-fe8b-4b92-96b2-849be6441281"
   },
   "outputs": [],
   "source": [
    "# create the first project and data collection\n",
    "project_name = \"Starter Example Companion - Fairness\"\n",
    "tru.set_project(project_name)\n",
    "tru.set_data_collection(\"Data Collection v1\")\n",
    "tru.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc124eed-714d-488f-bdf2-a7e1ebe98788",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "bc124eed-714d-488f-bdf2-a7e1ebe98788",
    "outputId": "5f12cb56-14f8-439e-bea3-ce0e024785bd"
   },
   "outputs": [],
   "source": [
    "tru.set_model(tru.get_models()[0])\n",
    "tru.tester.get_model_test_results(test_types=[\"fairness\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "062b0f35-8d10-4fd9-a8f4-59b9b2474267",
   "metadata": {
    "id": "062b0f35-8d10-4fd9-a8f4-59b9b2474267"
   },
   "source": [
    "* What? Shown in the model test results, the first version of the test fails the Impact Ratio Test.\n",
    "\n",
    "* Why? After exploring results, CA has a better impact ratio than in NY, leading us to a hypothesis that CA might have a lower difference in ground truth outcomes between men and women. This phenomenon can also be referred to as *dataset disparity*.\n",
    "\n",
    "* We can examine the dataset disparity either in the web app or in the SDK (shown below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc2117",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63dc2117",
    "outputId": "ee46048c-2fe3-4c53-e54a-bebce3b3f7c0"
   },
   "outputs": [],
   "source": [
    "# dataset disparity for 2014-NY\n",
    "explainer = tru.get_explainer(\"2014-NY\")\n",
    "explainer.set_segment(segment_group_name=\"Sex\", segment_name=\"Female\")\n",
    "mean_outcome_females_NY_2014 = explainer.get_ys().mean()\n",
    "explainer.set_segment(segment_group_name=\"Sex\", segment_name=\"Male\")\n",
    "mean_outcome_males_NY_2014 = explainer.get_ys().mean()\n",
    "print(\"2014-NY dataset disparity: \" + str(mean_outcome_females_NY_2014 - mean_outcome_males_NY_2014))\n",
    "\n",
    "# dataset disparity for 2014-CA\n",
    "explainer = tru.get_explainer(\"2014-CA\")\n",
    "explainer.set_segment(segment_group_name=\"Sex\", segment_name=\"Female\")\n",
    "mean_outcome_females_CA_2014 = explainer.get_ys().mean()\n",
    "explainer.set_segment(segment_group_name=\"Sex\", segment_name=\"Male\")\n",
    "mean_outcome_males_CA_2014 = explainer.get_ys().mean()\n",
    "print(\"2014-CA dataset disparity: \" + str(mean_outcome_females_CA_2014 - mean_outcome_males_CA_2014))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d97769ad",
   "metadata": {
    "id": "d97769ad"
   },
   "source": [
    "This calculation of dataset disparity for NY and CA confirm our hypothesis.\n",
    "\n",
    "Going forward, we should retrain our model on CA with the belief that more fair training data will lead to a more fair model. But let's not stop there.\n",
    "\n",
    "* We notice that sex is included in the training data, and the second leading contributor to disparity. We should remove it so it's not learned by the model.\n",
    "\n",
    "* What next? Let's see the fairness outcome after removing sex from the training data and using CA as our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ac911",
   "metadata": {
    "id": "c47ac911"
   },
   "outputs": [],
   "source": [
    "# get data and feature map\n",
    "\n",
    "from smart_open import open\n",
    "\n",
    "data_s3_file_name = \"https://truera-examples.s3.us-west-2.amazonaws.com/data/starter-fairness/starter-fairness-data-compressed.pickle\"\n",
    "with open(data_s3_file_name, \"rb\") as f:\n",
    "    data = pd.read_pickle(f)\n",
    "\n",
    "feature_map_s3_file_name = \"https://truera-examples.s3.us-west-2.amazonaws.com/data/starter-fairness/starter-fairness-feature-map.pickle\"\n",
    "with open(feature_map_s3_file_name, \"rb\") as f:\n",
    "    feature_map = pd.read_pickle(f)\n",
    "feature_map.pop(\"Sex\")\n",
    "\n",
    "tru.add_data_collection(\"Data Collection v2\", pre_to_post_feature_map=feature_map, provide_transform_with_model=False)\n",
    "\n",
    "# add data splits to the collection we just created\n",
    "year_begin = 2014\n",
    "year_end = 2016  # exclusive\n",
    "states = [\"CA\", \"NY\"]\n",
    "\n",
    "for year in range(year_begin, year_end):\n",
    "    for state in states:\n",
    "        data_split_name = f\"{year}-{state}\"\n",
    "        print(f\"Ingesting data-split: {data_split_name}...\")\n",
    "        ids = [f\"{i}\" for i in range(len(data[year][state][\"data_preprocessed\"]))]\n",
    "        data[year][state][\"data_preprocessed\"][\"id\"] = ids\n",
    "        data[year][state][\"data_postprocessed\"][\"id\"] = ids\n",
    "        data[year][state][\"label\"] = pd.DataFrame(data[year][state][\"label\"])\n",
    "        data[year][state][\"label\"][\"id\"] = ids\n",
    "        data[year][state][\"extra_data\"] = pd.DataFrame(data[year][state][\"extra_data\"][[\"LANX\"]])\n",
    "        data[year][state][\"extra_data\"][\"id\"] = ids\n",
    "        # data\n",
    "        merged_data = data[year][state][\"data_preprocessed\"].\\\n",
    "            merge(data[year][state][\"data_postprocessed\"]).\\\n",
    "            merge(data[year][state][\"label\"]).\\\n",
    "            merge(data[year][state][\"extra_data\"])\n",
    "        tru.add_data(\n",
    "            data=merged_data,\n",
    "            data_split_name=data_split_name,\n",
    "            column_spec=ColumnSpec(\n",
    "                id_col_name=\"id\",\n",
    "                pre_data_col_names=list(data[year][state][\"data_preprocessed\"].columns.drop(\n",
    "                    [\"id\", \"Sex\"])),  # drop sex from pre-data\n",
    "                post_data_col_names=list(data[year][state][\"data_postprocessed\"].columns.drop(\n",
    "                    [\"id\", \"Sex_Male\", \"Sex_Female\"])),\n",
    "                label_col_names=list(data[year][state][\"label\"].columns.drop(\"id\")),\n",
    "                extra_data_col_names=list(data[year][state][\"extra_data\"].columns.drop(\"id\")) +\n",
    "                [\"Sex\"]  # add sex to extra-data\n",
    "            ))\n",
    "tru.set_influences_background_data_split(f\"{year_begin}-{states[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Oc61CAfctHTI",
   "metadata": {
    "id": "Oc61CAfctHTI"
   },
   "outputs": [],
   "source": [
    "# train xgboost\n",
    "models = {}\n",
    "model_name_v2 = \"model_2\"\n",
    "\n",
    "models[model_name_v2] = xgb.XGBClassifier(eta=0.2, max_depth=4)\n",
    "\n",
    "models[model_name_v2].fit(data[2014][\"CA\"][\"data_postprocessed\"].drop([\"Sex_Female\", \"Sex_Male\", \"id\"], axis=1),\n",
    "                          data[2014][\"CA\"][\"label\"].drop(\"id\", axis=1))\n",
    "\n",
    "train_params = {\"model_type\": \"xgb.XGBClassifier\", \"eta\": 0.2, \"max_depth\": 4}\n",
    "\n",
    "# ingest the model\n",
    "tru.set_project(project_name)\n",
    "tru.set_data_collection(\"Data Collection v2\")\n",
    "tru.add_python_model(model_name_v2, models[model_name_v2], train_split_name=\"2014-CA\", train_parameters=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82af504",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.set_model(\"model_2\")\n",
    "tru.set_influences_background_data_split(\"2014-NY\")\n",
    "\n",
    "# set model output context\n",
    "model_output_context = ModelOutputContext(model_name=model_name_v2,\n",
    "                                          score_type=\"probits\",\n",
    "                                          background_split_name=f\"2014-NY\",\n",
    "                                          influence_type='tree-shap-interventional')\n",
    "\n",
    "for year in range(year_begin, year_end):\n",
    "    for state in states:\n",
    "        data_split_name = f\"{year}-{state}\"\n",
    "        tru.set_data_split(data_split_name)\n",
    "        print(f\"Ingesting data-split: {data_split_name}...\")\n",
    "        preds = tru.get_ys_pred().reset_index(names=\"id\")\n",
    "        # predictions\n",
    "        tru.add_data(data=preds,\n",
    "                     data_split_name=data_split_name,\n",
    "                     column_spec=ColumnSpec(id_col_name=\"id\", prediction_col_names=\"__truera_prediction__\"))\n",
    "        # influences\n",
    "        tru.compute_feature_influences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94f1e7-da04-470f-8f95-abba9983f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view model test results\n",
    "tru.set_model(model_name_v2)\n",
    "tru.tester.get_model_test_results(test_types=[\"fairness\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "799c1964",
   "metadata": {
    "id": "799c1964"
   },
   "source": [
    "* What? Our newest model now passes 2/4 impact ratio tests.\n",
    "* Shown on the Dataset Disparity section of the Fairness page in the web app, we notice that there is a lower positive outcome rate for females in the data. Let's check the performance of females with a positive label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.set_model(model_name_v2)\n",
    "tru.set_data_split(\"2014-CA\")\n",
    "tru.add_segment_group(\n",
    "    \"Sex + Label\", {\n",
    "        \"Other\": \"(Sex == 'Male') OR (_DATA_GROUND_TRUTH == 0)\",\n",
    "        \"Female + Label 1\": \"(Sex == 'Female') AND (_DATA_GROUND_TRUTH == 1)\"\n",
    "    })\n",
    "explainer = tru.get_explainer(base_data_split=\"2014-CA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.set_segment(segment_group_name=\"Sex + Label\", segment_name=\"Female + Label 1\")\n",
    "print(\"Female + label 1 performance: \" + str(explainer.compute_performance(metric_type=\"CLASSIFICATION_ACCURACY\")))\n",
    "explainer.set_segment(segment_group_name=\"Sex + Label\", segment_name=\"Other\")\n",
    "print(\"Other performance: \" + str(explainer.compute_performance(metric_type=\"CLASSIFICATION_ACCURACY\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7edb6de",
   "metadata": {
    "id": "a7edb6de"
   },
   "source": [
    "Our hunch was correct. To correct for this, rebalance the training set by overampling female with label==1 in train set because female with label==1 has more error than rest_of_pop with label==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53005b-136c-47d5-896b-d752b331368b",
   "metadata": {
    "id": "1c53005b-136c-47d5-896b-d752b331368b"
   },
   "outputs": [],
   "source": [
    "def rebalance_gender(df, data_type):\n",
    "    if data_type == 0:\n",
    "        df_female_true = df[(df[\"Sex\"] == \"Female\") & (df[\"PINCP\"] == True)]\n",
    "        df_else = df[~((df[\"Sex\"] == \"Female\") & (df[\"PINCP\"] == True))]\n",
    "    else:\n",
    "        df_female_true = df[(df[\"Sex_Female\"] == 1) & (df[\"PINCP\"] == True)]\n",
    "        df_else = df[~((df[\"Sex_Female\"] == 1) & (df[\"PINCP\"] == True))]\n",
    "\n",
    "    if data_type == 0:\n",
    "        num_samples = len(df[(df[\"Sex\"] == \"Male\") & (df[\"PINCP\"] == True)])\n",
    "    else:\n",
    "        num_samples = len(df[(df[\"Sex_Male\"] == 1) & (df[\"PINCP\"] == True)])\n",
    "    # Resample female target segment so that they are the same size as male\n",
    "    df_female_true_resampled = resample(\n",
    "        df_female_true,\n",
    "        replace=True,\n",
    "        n_samples=num_samples,\n",
    "        random_state=1  # include random seed so we can perform same sampling on each data set\n",
    "    )\n",
    "\n",
    "    return pd.concat([df_female_true_resampled, df_else])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f3c31",
   "metadata": {
    "id": "201f3c31"
   },
   "outputs": [],
   "source": [
    "data[2014][\"CA\"][\"data_preprocessed_resampled\"] = rebalance_gender(data[2014][\"CA\"][\"data_preprocessed\"].reset_index(drop=True).\\\n",
    "                                        merge(data[2014][\"CA\"][\"label\"].reset_index(drop=True), on=\"id\"), 0).drop([\"Sex\",\"PINCP\"], axis=1)\n",
    "data[2014][\"CA\"][\"data_postprocessed_resampled\"] = rebalance_gender(data[2014][\"CA\"][\"data_postprocessed\"].reset_index(drop=True).\\\n",
    "                                        merge(data[2014][\"CA\"][\"label\"].reset_index(drop=True), on=\"id\"), 1).drop([\"Sex_Male\",\"Sex_Female\", \"PINCP\"], axis=1)\n",
    "data[2014][\"CA\"][\"label_resampled\"] = rebalance_gender(pd.DataFrame(data[2014][\"CA\"][\"label\"].reset_index(drop=True)).\\\n",
    "                                        merge(data[2014][\"CA\"][\"data_preprocessed\"].reset_index(drop=True), on=\"id\"), 0).drop([\"Sex\"], axis=1)[[\"PINCP\",\"id\"]]\n",
    "data[2014][\"CA\"][\"extra_data_resampled\"] = rebalance_gender(pd.DataFrame(data[2014][\"CA\"][\"extra_data\"].reset_index(drop=True)).\\\n",
    "                                        merge(data[2014][\"CA\"][\"data_preprocessed\"].reset_index(drop=True), on=\"id\").\\\n",
    "                                        merge(data[2014][\"CA\"][\"label\"].reset_index(drop=True), on=\"id\"), 0)[[\"LANX\",\"id\",\"Sex\"]]\n",
    "\n",
    "ids = [f\"{i}\" for i in range(len(data[2014][\"CA\"][\"data_preprocessed_resampled\"]))]\n",
    "data[2014][\"CA\"][\"data_preprocessed_resampled\"][\"id\"] = ids\n",
    "data[2014][\"CA\"][\"data_postprocessed_resampled\"][\"id\"] = ids\n",
    "data[2014][\"CA\"][\"label_resampled\"][\"id\"] = ids\n",
    "data[2014][\"CA\"][\"extra_data_resampled\"][\"id\"] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = data[2014][\"CA\"][\"data_preprocessed_resampled\"].\\\n",
    "            merge(data[2014][\"CA\"][\"data_postprocessed_resampled\"]).\\\n",
    "            merge(data[2014][\"CA\"][\"label_resampled\"]).\\\n",
    "            merge(data[2014][\"CA\"][\"extra_data_resampled\"])\n",
    "tru.add_data(data=merged_data,\n",
    "             data_split_name=\"2014-CA-resampled\",\n",
    "             column_spec=ColumnSpec(\n",
    "                 id_col_name=\"id\",\n",
    "                 pre_data_col_names=list(data[2014][\"CA\"][\"data_preprocessed_resampled\"].columns.drop([\"id\"])),\n",
    "                 post_data_col_names=list(data[2014][\"CA\"][\"data_postprocessed_resampled\"].columns.drop([\"id\"])),\n",
    "                 label_col_names=list(data[2014][\"CA\"][\"label_resampled\"].columns.drop(\"id\")),\n",
    "                 extra_data_col_names=list(data[2014][\"CA\"][\"extra_data_resampled\"].columns.drop(\"id\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z0HEF2dmtU3U",
   "metadata": {
    "id": "z0HEF2dmtU3U"
   },
   "outputs": [],
   "source": [
    "# train xgboost\n",
    "model_name_v3 = \"model_3\"\n",
    "\n",
    "models[model_name_v3] = xgb.XGBClassifier(eta=0.2, max_depth=4)\n",
    "models[model_name_v3].fit(data[2014][\"CA\"][\"data_postprocessed_resampled\"].drop(\"id\", axis=1),\n",
    "                          data[2014][\"CA\"][\"label_resampled\"].drop(\"id\", axis=1))\n",
    "\n",
    "# ingest the model\n",
    "tru.add_python_model(model_name_v3,\n",
    "                     models[model_name_v3],\n",
    "                     train_split_name=\"2014-CA-resampled\",\n",
    "                     train_parameters=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.set_model(model_name_v3)\n",
    "tru.set_influences_background_data_split(\"2014-CA-resampled\")\n",
    "\n",
    "# set model output context\n",
    "model_output_context = ModelOutputContext(model_name=model_name_v3,\n",
    "                                          score_type=\"probits\",\n",
    "                                          background_split_name=f\"2014-CA-resampled\",\n",
    "                                          influence_type='tree-shap-interventional')\n",
    "\n",
    "data_split_name = \"2014-CA-resampled\"\n",
    "tru.set_data_split(data_split_name)\n",
    "print(f\"Ingesting data-split: 2014-CA-resampled...\")\n",
    "\n",
    "# predictions\n",
    "preds = tru.get_ys_pred().reset_index(names=\"id\")\n",
    "tru.add_data(data=preds,\n",
    "             data_split_name=data_split_name,\n",
    "             column_spec=ColumnSpec(id_col_name=\"id\", prediction_col_names=\"__truera_prediction__\"))\n",
    "\n",
    "# influences\n",
    "tru.compute_feature_influences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857dae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(year_begin, year_end):\n",
    "    for state in states:\n",
    "        data_split_name = f\"{year}-{state}\"\n",
    "        tru.set_data_split(data_split_name)\n",
    "        print(f\"Ingesting data-split: {data_split_name}...\")\n",
    "        preds = tru.get_ys_pred().reset_index(names=\"id\")\n",
    "        # predictions\n",
    "        tru.add_data(data=preds,\n",
    "                     data_split_name=data_split_name,\n",
    "                     column_spec=ColumnSpec(id_col_name=\"id\", prediction_col_names=\"__truera_prediction__\"))\n",
    "        # influences\n",
    "        tru.compute_feature_influences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104cee5-e74e-46b7-af73-2ab98cdbbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view model test results\n",
    "tru.set_model(model_name_v3)\n",
    "tru.tester.get_model_test_results(test_types=[\"fairness\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ddb76b1-c90b-4f7b-a684-1128412c4955",
   "metadata": {
    "id": "7ddb76b1-c90b-4f7b-a684-1128412c4955"
   },
   "source": [
    "### After rebalancing, the model passes 4/4 impact ratio tests, giving us confidence in the fairness of the model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ce855c64"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('eap_demo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "da3924fd47657b295ea4ff31ffc159b50bdc67549cd17755b5462d5d6bda3f74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
